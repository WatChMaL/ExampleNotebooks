{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN on MNIST\n",
    "\n",
    "In this notebook, we train one of oldest CNNs, called [_LeNet_](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), to classify hand-written digits from 0 to 9. The dataset is public and called [_MNIST_](http://yann.lecun.com/exdb/mnist/). MNIST is widely used for an introductory machine learning (ML) courses/lectures, and not limited to CNN. LeNet is one of the very first CNN application and nearly 2 decades old, and first applied on MNIST.\n",
    "\n",
    "First we go over MNIST data set, LeNet construction, and finally training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set\n",
    "Most, if not all, ML libraries provide an easy way (API) to access MNIST dataset. This is true in `pytorch` as well. MNIST dataset in `Dataset` instance is available from `torchvision`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 images prepared!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "LOCAL_DATA_DIR = './data' # this is where data files are downloaded\n",
    "\n",
    "dataset = datasets.MNIST(LOCAL_DATA_DIR, train=True, download=True,\n",
    "                         transform=transforms.Compose([transforms.ToTensor()]))\n",
    "print(len(dataset),'images prepared!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize data with a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Print label\n",
    "print('Label:',dataset[0][1])\n",
    "# Draw data\n",
    "plt.imshow(dataset[0][0].view([28,28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's crate `DataLoader` for training. If you have not learned what `DataLoader` is, [read this notebook](https://github.com/watchmal/ExampleNotebooks/blob/master/HKML%20DataLoader.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=20,shuffle=True,num_workers=1,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a network\n",
    "In `pytorch`, a network is defined as a `torch.Module` inherited class. All `torch.Module` have two functions: _forward_ and _backward_. The forward function runs data through the CNN and makes a prediction. The backward propagates gradient, computed based on the error from the forward function call, to train the network. \n",
    "\n",
    "LeNet has 2 convolution layers, each followed by rectified linear unit (ReLU) and max-pooling, for feature extractions. Then the extracted features run through two hidden layers multi-layer perceptron (MLP) at the end for classifying the input image data into digits (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LeNet, self).__init__()\n",
    "        # feature extractor CNN\n",
    "        self._feature_extractor = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,6,5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(6,16,5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2) )\n",
    "        # classifier MLP\n",
    "        self._classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256,120),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(120,84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(84,10) )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # extract features\n",
    "        features = self._feature_extractor(x)\n",
    "        # flatten the 3d tensor (2d space x channels = features)\n",
    "        features = features.view(-1, np.prod(features.size()[1:]))\n",
    "        # classify and return\n",
    "        return self._classifier(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a train loop\n",
    "For convenience, define a _BLOB_ class to keep objects together. To a BLOB instance, we attach LeNet, our loss function (`nn.CrossEntropyLoss`), and Adam optimizer algorithm. For analysis purpose, we also include `nn.Softmax`. Finally, we attach data and label place holders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLOB:\n",
    "    pass\n",
    "blob=BLOB()\n",
    "blob.net       = LeNet().cuda() # construct Lenet, use GPU\n",
    "blob.criterion = torch.nn.CrossEntropyLoss() # use softmax loss to define an error\n",
    "blob.optimizer = torch.optim.Adam(blob.net.parameters()) # use Adam optimizer algorithm\n",
    "blob.softmax   = torch.nn.Softmax(dim=1) # not for training, but softmax score for each class\n",
    "blob.data      = None # data for training/analysis\n",
    "blob.label     = None # label for training/analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define 2 functions to be called in the training loop: forward and backward. These functions implement the evaluation of the results, error (loss) definition, and propagation of errors (gradients) back to update the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(blob,train=True):\n",
    "    \"\"\"\n",
    "       Args: blob should have attributes, net, criterion, softmax, data, label\n",
    "       \n",
    "       Returns: a dictionary of predicted labels, softmax, loss, and accuracy\n",
    "    \"\"\"\n",
    "    with torch.set_grad_enabled(train):\n",
    "        # Prediction\n",
    "        data = blob.data.cuda()\n",
    "        prediction = blob.net(data)\n",
    "        # Training\n",
    "        loss,acc=-1,-1\n",
    "        if blob.label is not None:\n",
    "            label = blob.label.cuda() #label = torch.stack([ torch.as_tensor(l) for l in np.hstack(label) ])\n",
    "            label.requires_grad = False\n",
    "            loss = blob.criterion(prediction,label)\n",
    "        blob.loss = loss\n",
    "        \n",
    "        softmax    = blob.softmax(prediction).cpu().detach().numpy()\n",
    "        prediction = torch.argmax(prediction,dim=-1)\n",
    "        accuracy   = (prediction == label).sum().item() / float(prediction.nelement())        \n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        \n",
    "        return {'prediction' : prediction,\n",
    "                'softmax'    : softmax,\n",
    "                'loss'       : loss,#.cpu().detatch(),\n",
    "                'accuracy'   : accuracy}\n",
    "\n",
    "def backward(blob):\n",
    "    blob.optimizer.zero_grad()  # Reset gradients accumulation\n",
    "    blob.loss.backward()\n",
    "    blob.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for training! Let's write a loop to load data, call forward, and call backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99 ... Loss tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.85\n",
      "Iteration 199 ... Loss tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.95\n",
      "Iteration 299 ... Loss tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 399 ... Loss tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.8\n",
      "Iteration 499 ... Loss tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.9\n",
      "Iteration 599 ... Loss tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.9\n",
      "Iteration 699 ... Loss tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.95\n",
      "Iteration 799 ... Loss tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 899 ... Loss tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 999 ... Loss tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.95\n",
      "Iteration 1099 ... Loss tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 1199 ... Loss tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.95\n",
      "Iteration 1299 ... Loss tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 1399 ... Loss tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.95\n",
      "Iteration 1499 ... Loss tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 1599 ... Loss tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 0.85\n",
      "Iteration 1699 ... Loss tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 1799 ... Loss tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 1899 ... Loss tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n",
      "Iteration 1999 ... Loss tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>) ... Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Set the network to training mode\n",
    "blob.net.train()\n",
    "\n",
    "# Let's train 2000 steps\n",
    "stop_iteration = 2000\n",
    "# Loop over data samples and into the network forward function\n",
    "for i,data in enumerate(train_loader):\n",
    "    # data and label\n",
    "    blob.data, blob.label = data\n",
    "    # call forward\n",
    "    res = forward(blob,True)\n",
    "    # once in a while, report\n",
    "    if (i+1)%100 == 0:\n",
    "        print('Iteration',i,'... Loss',res['loss'],'... Accuracy',res['accuracy'])\n",
    "    if (i+1)==stop_iteration:\n",
    "        break\n",
    "    backward(blob)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "It seems that the network was trained well on the data set. Let's evaluate its performance on a separate data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean: 0.9708 ... std: 0.04286443747443795\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "dataset = datasets.MNIST(LOCAL_DATA_DIR, train=False, download=True,\n",
    "                         transform=transforms.Compose([transforms.ToTensor()]))\n",
    "# create DataLoader\n",
    "test_loader = torch.utils.data.DataLoader(dataset,batch_size=20,shuffle=False,num_workers=1,pin_memory=True)\n",
    "# set the network to test (non-train) mode\n",
    "blob.net.eval()\n",
    "# loop over test set, run forward for analysis\n",
    "accuracy_array = []\n",
    "for i,data in enumerate(test_loader):\n",
    "    blob.data, blob.label = data\n",
    "    res = forward(blob,True)\n",
    "    accuracy_array.append(res['accuracy'])\n",
    "\n",
    "# report accuracy\n",
    "accuracy_array = np.hstack(accuracy_array)\n",
    "mean = accuracy_array.mean()\n",
    "std  = accuracy_array.std()\n",
    "print('Accuracy mean:',mean,'... std:',std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
